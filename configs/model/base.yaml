model:
  vocab_size: 32000
  d_model: 512
  num_encoder_layers: 6
  num_decoder_layers: 6
  num_heads: 8
  d_ff: 2048
  dropout: 0.1
  max_seq_length: 512
  
tasks:
  summarization:
    enabled: true
    decoder_layers: 6
    
  emotion:
    enabled: true
    num_classes: 27
    pool_strategy: "mean"  # Options: mean, max, cls, attention
    
  clustering:
    enabled: true
    embedding_dim: 128
    normalize: true

training:
  batch_size: 16
  gradient_accumulation_steps: 2  # Effective batch = 32
  learning_rate: 1e-4
  weight_decay: 0.01
  num_epochs: 10
  warmup_steps: 1000
  max_grad_norm: 1.0
  
  scheduler:
    type: "cosine"  # Options: linear, cosine, polynomial
    
  mixed_precision: true  # Use AMP for faster training

data:
  max_length: 512
  summary_max_length: 128
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  
  preprocessing:
    lowercase: true
    remove_stopwords: false
    min_token_length: 3