# Full Training Configuration for FLAN-T5-base
# Complete training run on all data
# Training time: ~6-8 hours on RTX 4070
# Use: python scripts/train.py training=full

dataloader:
  batch_size: 11  # Reduced for FLAN-T5-base (12 layers)
  shuffle: true
  num_workers: 8
  pin_memory: true

optimizer:
  name: adamw
  lr: 2.0e-5
  weight_decay: 0.01

scheduler:
  name: cosine
  warmup_steps: 1000  # More warmup for full training

trainer:
  max_epochs: 4
  gradient_clip_norm: 0.5
  gradient_accumulation_steps: 6  # Effective batch size = 8 * 6 = 48
  validation_max_length: 128
  label_smoothing: 0.1
  task_weights:
    summarization: 1.0
    emotion: 1.0
    topic: 1.0
