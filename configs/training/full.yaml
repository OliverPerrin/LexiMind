# Full Training Configuration for FLAN-T5-base
# Complete training run on all data
# Training time: ~6-8 hours on RTX 4070 12GB
# Use: python scripts/train.py training=full

dataloader:
  batch_size: 6          # Optimized for 12GB VRAM
  shuffle: true
  num_workers: 6
  pin_memory: true

optimizer:
  name: adamw
  lr: 2.0e-5
  weight_decay: 0.01

scheduler:
  name: cosine
  warmup_steps: 500       # ~3% of steps

trainer:
  max_epochs: 3           # 3 epochs usually sufficient, avoids overfit
  gradient_clip_norm: 1.0
  gradient_accumulation_steps: 6  # Effective batch = 36
  validation_max_length: 128
  label_smoothing: 0.1
  task_weights:
    summarization: 1.0
    emotion: 1.0
    topic: 1.0
