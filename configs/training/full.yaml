# Full Training Configuration for FLAN-T5-base
# Complete training run on all available data
# VRAM Usage: ~10-11GB peak (12GB available)
# Training time: ~3-4 hours on RTX 4070 12GB with torch.compile
# Use: python scripts/train.py training=full

dataloader:
  batch_size: 6  # Conservative for 12GB VRAM with torch.compile overhead
  shuffle: true
  num_workers: 4
  pin_memory: true
  persistent_workers: true
  prefetch_factor: 2

optimizer:
  name: adamw
  lr: 3.0e-5  # Higher LR with larger effective batch
  weight_decay: 0.01
  eps: 1.0e-6
  betas: [0.9, 0.999]

scheduler:
  name: cosine
  warmup_steps: 1000  # ~1% warmup for stability

trainer:
  max_epochs: 8  # More epochs for full dataset
  gradient_clip_norm: 1.0
  gradient_accumulation_steps: 16  # Effective batch: 96 (6*16)
  validation_max_length: 128
  label_smoothing: 0.1
  task_weights:
    summarization: 1.5  # Prioritize summarization quality
    emotion: 1.0
    topic: 0.8
  # No max_samples - use full dataset
  early_stopping_patience: 3  # Stop if plateaus
  log_grad_norm_frequency: 100

# Enable torch.compile for maximum speed
compile_encoder: true
compile_decoder: true

tokenizer_max_length: 512
