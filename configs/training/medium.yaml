# Medium Configuration for FLAN-T5-base
# Balanced approach - good results in reasonable time
# Training time: ~2-3 hours on RTX 4070
# Use: python scripts/train.py training=medium
# Note: FLAN-T5-base has 12 layers (vs BART's 6), may need smaller batch

dataloader:
  batch_size: 11  # Reduced for FLAN-T5-base (12 layers uses more VRAM)
  shuffle: true
  num_workers: 8
  pin_memory: true

optimizer:
  name: adamw
  lr: 2.0e-5  # Slightly lower for larger model
  weight_decay: 0.01

scheduler:
  name: cosine
  warmup_steps: 500  # More warmup for larger model

trainer:
  max_epochs: 3
  gradient_clip_norm: 0.5
  gradient_accumulation_steps: 4  # Effective batch size = 8 * 4 = 32
  validation_max_length: 128
  label_smoothing: 0.1
  task_weights:
    summarization: 1.0
    emotion: 1.0
    topic: 1.0
  
  # Medium dataset - good representative sample
  max_train_samples: 50000
  max_val_samples: 5000
  validation_frequency: 5000