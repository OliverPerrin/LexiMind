# Medium Configuration for FLAN-T5-base
# Balanced approach - good results in reasonable time
# Training time: ~2-3 hours on RTX 4070 12GB
# Use: python scripts/train.py training=medium

dataloader:
  batch_size: 6          # Optimized for 12GB VRAM with accumulation
  shuffle: true
  num_workers: 6
  pin_memory: true

optimizer:
  name: adamw
  lr: 3.0e-5              # Slightly higher - compensates for effective batch
  weight_decay: 0.01

scheduler:
  name: cosine
  warmup_steps: 300       # ~5% of steps

trainer:
  max_epochs: 3
  gradient_clip_norm: 1.0
  gradient_accumulation_steps: 3  # Effective batch = 18
  validation_max_length: 96
  label_smoothing: 0.1
  task_weights:
    summarization: 1.0
    emotion: 1.0
    topic: 1.0
  max_train_samples: 50000
  max_val_samples: 5000