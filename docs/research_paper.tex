% LexiMind: Multi-Task Learning for Literary and Academic Text Understanding
% Research Paper - Revised with Experimental Rigor
% Author: Oliver Perrin

\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

% Essential packages
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{caption}

% TikZ for diagrams
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows, positioning}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue
}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Multi-Task Learning for Literary and Academic Text:\\Does Joint Training Help or Hurt?}

\author{\IEEEauthorblockN{Oliver Perrin}\\
\IEEEauthorblockA{Department of Computer Science\\
Appalachian State University\\
Email: perrinot@appstate.edu}}

\maketitle

\begin{abstract}
Multi-task learning (MTL) promises improved generalization through shared representations, but its benefits depend heavily on task relatedness and domain characteristics. We investigate whether MTL improves performance on literary and academic text understanding---domains underrepresented in existing benchmarks dominated by news articles. Using a FLAN-T5-base encoder-decoder backbone (272M parameters), we jointly train on three tasks: abstractive summarization (49K samples: full-text passages $\rightarrow$ descriptive summaries from Goodreads book descriptions and arXiv abstracts), topic classification (3.4K samples across 7 categories), and multi-label emotion detection (43K samples from GoEmotions). Through ablation studies comparing single-task specialists against multi-task configurations, we find that: (1) MTL provides a +3.2\% accuracy boost for topic classification due to shared encoder representations from the larger summarization corpus, (2) summarization quality remains comparable (BERTScore F1 0.83 vs. 0.82 single-task), and (3) emotion detection suffers negative transfer ($-$0.02 F1), which we attribute to domain mismatch between Reddit-sourced emotion labels and literary/academic text, compounded by the 28-class multi-label sparsity and the use of an encoder-decoder (rather than encoder-only) backbone. We further ablate the contribution of FLAN-T5 pre-training versus random initialization, finding that transfer learning accounts for the majority of final performance across all tasks. Our analysis reveals that MTL benefits depend critically on dataset size ratios, domain alignment, and architectural isolation of task-specific components, offering practical guidance for multi-task system design. We note limitations in statistical power (single-seed results on a small topic dataset) and the absence of gradient-conflict mitigation methods such as PCGrad, which we identify as important future work.
\end{abstract}

\begin{IEEEkeywords}
Multi-Task Learning, Transfer Learning, Text Summarization, Emotion Classification, FLAN-T5
\end{IEEEkeywords}

%=============================================================================
\section{Introduction}
%=============================================================================

Multi-task learning (MTL) \cite{caruana1997multitask} trains a single model on multiple related tasks, hypothesizing that shared representations improve generalization. In NLP, MTL has shown promise for sequence labeling \cite{collobert2011natural}, machine translation \cite{johnson2017google}, and question answering \cite{mccann2018natural}. However, recent work highlights that MTL does not universally help---negative transfer can occur when tasks compete for model capacity \cite{standley2020tasks}, and gradient conflicts between tasks can degrade joint optimization \cite{yu2020gradient}.

We investigate MTL effectiveness in a specific, underexplored domain: \textbf{literary and academic text understanding}. Unlike news articles---which dominate existing benchmarks like CNN/DailyMail \cite{nallapati2016abstractive} and XSum \cite{narayan2018don}---literary and academic texts exhibit distinct characteristics: longer context dependencies, domain-specific vocabulary, and different summary styles (descriptive abstracts vs. extractive headlines). Recent domain-specific summarization work, including BookSum \cite{kryscinski2021booksum} for narrative summarization and CiteSum \cite{mao2022citesum} for citation-contextualized scientific summaries, demonstrates that domain matters for summarization quality---yet multi-task learning effects within these domains remain unstudied.

Our study addresses three research questions:

\begin{enumerate}
    \item[\textbf{RQ1}] Does multi-task learning improve performance over single-task specialists on literary/academic domains?
    \item[\textbf{RQ2}] Which tasks benefit from joint training, and which suffer negative transfer?
    \item[\textbf{RQ3}] How much does pre-trained knowledge (FLAN-T5) contribute relative to task-specific fine-tuning?
\end{enumerate}

To answer these questions, we construct \textbf{LexiMind}, a multi-task system built on FLAN-T5-base \cite{chung2022scaling} that performs abstractive summarization, topic classification, and emotion detection. We conduct ablations comparing multi-task vs. single-task training, with vs. without FLAN-T5 initialization, and different task weight configurations. Our primary experimental contribution is the empirical characterization of transfer effects across these heterogeneous tasks:

\begin{itemize}
    \item \textbf{Topic classification benefits most from MTL} (+3.2\% accuracy), leveraging shared encoder representations from the larger summarization dataset.
    \item \textbf{Summarization is robust to MTL}, showing minimal change despite sharing encoder capacity with classification heads.
    \item \textbf{Emotion detection suffers negative transfer} ($-$0.02 F1), attributed to domain mismatch between GoEmotions' Reddit source and the formal literary/academic register.
    \item \textbf{Transfer learning dominates}: FLAN-T5 initialization provides the bulk of final performance; fine-tuning adds crucial domain adaptation.
\end{itemize}

We acknowledge important limitations: our results are from single-seed runs, we do not explore gradient-conflict mitigation methods (PCGrad \cite{yu2020gradient}, CAGrad \cite{liu2021conflict}), and our emotion evaluation conflates domain mismatch with multi-label threshold and architecture choices. We discuss these openly in Section~\ref{sec:limitations} and identify them as directions for future work.

%=============================================================================
\section{Related Work}
%=============================================================================

\subsection{Multi-Task Learning in NLP}

Collobert et al. \cite{collobert2011natural} demonstrated that joint training on POS tagging, chunking, and NER improved over single-task models. T5 \cite{raffel2020exploring} unified diverse NLP tasks through text-to-text framing, showing strong transfer across tasks. However, Standley et al. \cite{standley2020tasks} found that naive MTL often underperforms single-task learning, with performance depending on task groupings. More recently, Aghajanyan et al. \cite{aghajanyan2021muppet} showed that large-scale multi-task pre-finetuning can improve downstream performance, suggesting that the benefits of MTL depend on training scale and task diversity.

\textbf{Gradient conflict and loss balancing.} Yu et al. \cite{yu2020gradient} proposed PCGrad, which projects conflicting gradients to reduce interference, while Liu et al. \cite{liu2021conflict} introduced CAGrad for conflict-averse optimization. Chen et al. \cite{chen2018gradnorm} proposed GradNorm for dynamically balancing task losses based on gradient magnitudes. Kendall et al. \cite{kendall2018multi} explored uncertainty-based task weighting. Our work uses fixed loss weights---a simpler but less adaptive approach. We did not explore these gradient-balancing methods; the negative transfer we observe on emotion detection makes them a natural and important follow-up.

\textbf{Multi-domain multi-task studies.} Aribandi et al. \cite{aribandi2022ext5} studied extreme multi-task scaling and found that not all tasks contribute positively. Our work provides complementary evidence at smaller scale, showing that even within a three-task setup, transfer effects are heterogeneous and depend on domain alignment.

\subsection{Literary and Academic Summarization}

Most summarization benchmarks focus on news \cite{nallapati2016abstractive, narayan2018don}. BookSum \cite{kryscinski2021booksum} introduced chapter-level and book-level summarization for literary texts, but targets plot summaries rather than descriptive abstracts. arXiv summarization \cite{cohan2018discourse} addresses academic papers with discourse-aware models. CiteSum \cite{mao2022citesum} leverages citation sentences as summaries for scientific papers. Our summarization setup differs from these: we pair literary source passages (extracted from Project Gutenberg full texts, avg. 3,030 characters) with Goodreads book descriptions (avg. 572 characters) as targets, training the model to generate \textit{what a book is about} rather than plot recaps. For academic text, arXiv paper body text (avg. 3,967 characters) is paired with abstracts (avg. 1,433 characters). The resulting compression ratios (0.19 for literary, 0.36 for academic) are closer to genuine summarization than short paraphrasing.

\subsection{Emotion Detection}

GoEmotions \cite{demszky2020goemotions} provides 28 fine-grained emotion labels from Reddit comments. The original work reports 0.46 macro F1 using BERT-base with per-label thresholds tuned on the validation set. Subsequent work achieves 0.35--0.46 macro F1 depending on the model and threshold strategy. Importantly, all published GoEmotions baselines use encoder-only architectures (BERT, RoBERTa) rather than encoder-decoder models like T5. Our setup differs in both architecture (encoder-decoder with mean-pooled encoder states) and domain (training encoder primarily on literary/academic summarization), making direct comparison to published baselines informative but not fully controlled.

%=============================================================================
\section{Experimental Setup}
%=============================================================================

\subsection{Task Formulations}
\label{sec:task_formulation}

We define three tasks with explicit input-output specifications:

\textbf{Summarization (generative).} The input is a passage of source text; the target is a descriptive summary. For literary texts, the source is a passage from a Project Gutenberg full text (mean: 3,030 characters, truncated to 512 tokens), and the target is the corresponding Goodreads book description (mean: 572 characters)---a back-cover style blurb describing \textit{what the book is about}, not a plot recap. For academic texts, the source is a passage from an arXiv paper body (mean: 3,967 characters, truncated to 512 tokens), and the target is the paper's abstract (mean: 1,433 characters, truncated to 512 tokens). This formulation is closer to genuine document summarization than paraphrasing: the average compression ratios are 0.19 (literary) and 0.36 (academic), comparable to standard summarization benchmarks.

\textbf{Topic classification (discriminative, single-label).} The input is a text passage; the output is one of 7 classes: \textbf{Arts, Business, Fiction, History, Philosophy, Science, Technology}. Sources include 20 Newsgroups (mapped to our label taxonomy), Project Gutenberg subject metadata (for Fiction and Arts), and arXiv category metadata (for Science and Technology).

\textbf{Emotion detection (discriminative, multi-label).} The input is a text passage; the output is a subset of 28 emotion labels from GoEmotions \cite{demszky2020goemotions}. Labels are predicted via sigmoid activation with a fixed threshold of 0.3 during training evaluation and 0.5 during inference. We use a fixed threshold rather than per-class tuning; this simplifies the setup but likely underestimates achievable performance (see Section~\ref{sec:emotion_analysis}).

\subsection{Datasets}

Table \ref{tab:datasets} summarizes dataset statistics.

\begin{table}[htbp]
\centering
\caption{Dataset Statistics. Summarization sources are split approximately equally between literary and academic domains.}
\label{tab:datasets}
\begin{tabular}{llrrr}
\toprule
\textbf{Task} & \textbf{Source} & \textbf{Train} & \textbf{Val} & \textbf{Test} \\
\midrule
\multirow{2}{*}{Summarization} & Goodreads + Gutenberg & $\sim$4K & -- & -- \\
 & arXiv (body $\rightarrow$ abstract) & $\sim$45K & -- & -- \\
 & \textit{Combined} & 49,086 & 2,727 & 2,727 \\
\midrule
Topic (7 classes) & 20News + Gutenberg + arXiv & 3,402 & 189 & 189 \\
\midrule
Emotion (28 labels) & GoEmotions (Reddit) & 43,410 & 5,426 & 5,427 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Dataset curation.} Summarization pairs are constructed by matching Gutenberg full texts with Goodreads descriptions via title/author matching, and by pairing arXiv paper bodies with their abstracts. Text is truncated to 512 tokens (max encoder input length). No deduplication was performed across the literary and academic subsets, as they are drawn from disjoint sources. We note that the academic subset is substantially larger ($\sim$45K vs. $\sim$4K literary), creating a domain imbalance within the summarization task. Topic labels are derived from source metadata (arXiv categories, Gutenberg subjects, 20 Newsgroups categories) and mapped to our 7-class taxonomy; no manual annotation was performed. GoEmotions is used as-is from the HuggingFace datasets hub.

\textbf{Note on dataset sizes.} The large disparity between topic (3.4K) and summarization (49K) training sets is a key experimental variable: it tests whether a low-resource classification task can benefit from shared representations with a high-resource generative task.

\subsection{Model Architecture}

LexiMind uses FLAN-T5-base (272M parameters) as the backbone, with a custom reimplementation that loads pre-trained weights via a factory module for architectural transparency:

\begin{itemize}
    \item 12-layer encoder, 12-layer decoder
    \item 768-dimensional hidden states, 12 attention heads
    \item T5-style relative position bias (no absolute positional embeddings)
    \item Pre-Layer Normalization with RMSNorm \cite{zhang2019root}
    \item FlashAttention via PyTorch 2.0 SDPA when compatible
\end{itemize}

Task-specific heads branch from the shared encoder:
\begin{itemize}
    \item \textbf{Summarization}: Full decoder with language modeling head (cross-entropy loss with label smoothing)
    \item \textbf{Topic}: Linear classifier on mean-pooled encoder hidden states (cross-entropy loss)
    \item \textbf{Emotion}: Linear classifier on mean-pooled encoder hidden states with sigmoid activation (binary cross-entropy loss)
\end{itemize}

\textbf{Architectural note.} Using mean-pooled encoder states for classification in an encoder-decoder model is a pragmatic choice for parameter sharing, but may be suboptimal compared to encoder-only architectures (BERT, RoBERTa) where the encoder is fully dedicated to producing classification-ready representations. We discuss this trade-off in Section~\ref{sec:emotion_analysis}.

\subsection{Training Configuration}

All experiments use consistent hyperparameters unless otherwise noted:

\begin{itemize}
    \item \textbf{Optimizer}: Fused AdamW, lr=$3\times10^{-5}$, weight decay=0.01, $\beta_1$=0.9, $\beta_2$=0.98
    \item \textbf{Batch size}: 10 per step $\times$ 4 gradient accumulation = 40 effective
    \item \textbf{Schedule}: 300-step linear warmup, cosine decay to 0.1$\times$ peak lr
    \item \textbf{Max epochs}: 8 with early stopping (patience=3 on validation loss)
    \item \textbf{Precision}: BFloat16 on NVIDIA RTX 4070 (12GB VRAM)
    \item \textbf{Gradient clipping}: Max norm 1.0
    \item \textbf{Encoder freezing}: Bottom 4 layers frozen for stable transfer learning
\end{itemize}

\textbf{Task scheduling.} We use round-robin scheduling: at each training step, the model processes one batch from \textit{each} task sequentially, accumulating gradients before the optimizer step. This ensures all tasks receive equal update frequency regardless of dataset size. We did not explore alternative scheduling strategies (proportional sampling, temperature-based sampling), which is a limitation---proportional or temperature-based sampling could alter optimization dynamics, particularly for the small topic dataset.

\textbf{Loss weighting.} Task losses are combined with fixed weights: summarization=1.0, emotion=1.0, topic=0.3. The reduced topic weight was chosen to prevent the small topic dataset (3.4K samples, exhausted in $\sim$85 steps) from dominating gradients through rapid overfitting. We did not explore dynamic weighting methods such as GradNorm \cite{chen2018gradnorm} or uncertainty weighting \cite{kendall2018multi}; given the negative transfer observed on emotion, these methods could potentially improve results and are identified as future work.

\subsection{Baselines and Ablations}

We compare four configurations:

\begin{enumerate}
    \item \textbf{Random/Majority}: Random predictions for classification; for summarization, BERTScore is computed against the reference using a fixed output ``Summary not available'' (producing a baseline that reflects only the BERTScore model's behavior on unrelated text pairs---see Section~\ref{sec:baseline_discussion} for discussion).
    \item \textbf{FLAN-T5-base (zero-shot)}: Pre-trained model with task-appropriate prompts, no fine-tuning.
    \item \textbf{Single-Task}: Separate models fine-tuned on each task individually with identical hyperparameters. The single-task summarization model uses only the summarization dataset; topic and emotion models use only their respective datasets.
    \item \textbf{Multi-Task (LexiMind)}: Joint training on all three tasks with round-robin scheduling.
\end{enumerate}

We additionally ablate FLAN-T5 initialization vs. random initialization to isolate transfer learning contribution.

\subsection{Evaluation Metrics}

\begin{itemize}
    \item \textbf{Summarization}: ROUGE-1/2/L \cite{lin2004rouge} (lexical overlap) and BERTScore F1 \cite{zhang2019bertscore} using RoBERTa-large (semantic similarity). We report BERTScore as the primary metric because abstractive summarization produces paraphrases that ROUGE systematically undervalues.
    \item \textbf{Topic}: Accuracy and Macro F1 (unweighted average across 7 classes).
    \item \textbf{Emotion}: Sample-averaged F1 (computed per-sample as the harmonic mean of per-sample precision and recall, then averaged across all samples). We acknowledge that macro F1 (averaged per-class) and micro F1 (aggregated across all predictions) would provide complementary views; these are not reported in our current evaluation but are discussed in Section~\ref{sec:emotion_analysis}.
\end{itemize}

\textbf{Statistical note.} All results are from single training runs. We do not report confidence intervals or variance across seeds. Given the small topic dataset (189 validation samples), the observed +3.2\% accuracy improvement could be within random variance. We flag this as a limitation and recommend multi-seed evaluation for any production deployment.

%=============================================================================
\section{Results}
%=============================================================================

\subsection{Main Results: Multi-Task vs. Single-Task}

Table \ref{tab:main_results} compares MTL against single-task specialists.

\begin{table}[htbp]
\centering
\caption{Main Results: Multi-Task vs. Single-Task Performance. All results are single-seed. Bold indicates better performance between the two configurations.}
\label{tab:main_results}
\begin{tabular}{llcc}
\toprule
\textbf{Task} & \textbf{Metric} & \textbf{Single-Task} & \textbf{Multi-Task} \\
\midrule
\multirow{4}{*}{Summarization} & ROUGE-1 & 0.298 & \textbf{0.306} \\
 & ROUGE-2 & 0.085 & \textbf{0.090} \\
 & ROUGE-L & 0.179 & \textbf{0.183} \\
 & BERTScore F1 & 0.821 & \textbf{0.830} \\
\midrule
\multirow{2}{*}{Topic} & Accuracy & 82.0\% & \textbf{85.2\%} \\
 & Macro F1 & 0.812 & \textbf{0.847} \\
\midrule
Emotion & Sample-avg F1 & \textbf{0.218} & 0.199 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key finding}: MTL provides heterogeneous effects across tasks:

\begin{itemize}
    \item \textbf{Topic classification gains +3.2\% accuracy} from MTL. The small topic dataset (3.4K samples) benefits from shared encoder representations learned from the larger summarization corpus (49K samples). This is consistent with known benefits of MTL for low-resource tasks \cite{caruana1997multitask}. However, given the small validation set (189 samples), this gain corresponds to approximately 6 additional correct predictions---within plausible variance without multi-seed confirmation.
    
    \item \textbf{Summarization shows modest improvement} (+0.009 BERTScore F1). The generative task is robust to sharing encoder capacity with classification heads, likely because the decoder---which contains half the model's parameters---remains task-specific and insulates summarization from classification interference.
    
    \item \textbf{Emotion detection degrades by $-$0.019 F1}. This negative transfer is consistent with domain mismatch: GoEmotions labels derive from informal Reddit comments, while our encoder representations are shaped by formal literary/academic text. However, this also conflates with other factors (Section~\ref{sec:emotion_analysis}).
\end{itemize}

\subsection{Baseline Comparisons}
\label{sec:baseline_discussion}

Table \ref{tab:baselines} contextualizes our results against trivial and zero-shot baselines.

\begin{table}[htbp]
\centering
\caption{Comparison with Baselines}
\label{tab:baselines}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Summ (BS-F1)} & \textbf{Topic (Acc)} & \textbf{Emot (F1)} \\
\midrule
Random/Majority & 0.412 & 14.3\% & 0.036 \\
FLAN-T5 zero-shot & 0.724 & 58.2\% & 0.089 \\
Single-Task & 0.821 & 82.0\% & 0.218 \\
\textbf{Multi-Task} & \textbf{0.830} & \textbf{85.2\%} & 0.199 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{On the random baseline BERTScore (0.412).} BERTScore computes cosine similarity between contextual embeddings from RoBERTa-large. Even unrelated text pairs produce non-zero similarity because (a) common function words and subword tokens share embedding space, and (b) RoBERTa's embeddings have a non-zero mean that inflates cosine similarity. The 0.412 baseline reflects this ``floor'' effect rather than any meaningful semantic overlap. This is consistent with Zhang et al.'s \cite{zhang2019bertscore} observation that BERTScore baselines vary by language and domain.

Fine-tuning provides substantial gains over zero-shot across all tasks (+0.106 BERTScore, +27\% topic accuracy, +0.11 emotion F1), demonstrating the importance of domain adaptation even with instruction-tuned models.

\subsection{Ablation: Transfer Learning Contribution}

Table \ref{tab:transfer_ablation} isolates the contribution of FLAN-T5 pre-training by comparing against random initialization with identical architecture and training.

\begin{table}[htbp]
\centering
\caption{Effect of Pre-trained Initialization (Multi-Task Setting)}
\label{tab:transfer_ablation}
\begin{tabular}{lccc}
\toprule
\textbf{Initialization} & \textbf{Summ (BS-F1)} & \textbf{Topic (Acc)} & \textbf{Emot (F1)} \\
\midrule
Random & 0.523 & 45.2\% & 0.082 \\
FLAN-T5-base & \textbf{0.830} & \textbf{85.2\%} & \textbf{0.199} \\
\midrule
\textit{Absolute gain} & +0.307 & +40.0\% & +0.117 \\
\bottomrule
\end{tabular}
\end{table}

FLAN-T5 initialization provides large absolute gains across all tasks. We initially characterized this as ``85\% of final performance,'' but this framing oversimplifies heterogeneous metrics: BERTScore, accuracy, and F1 have different scales and baselines, making percentage attribution across them misleading. A more precise characterization: \textbf{pre-training is necessary for competitive performance}---random initialization produces substantially worse results on all tasks even with identical data and training budget. Fine-tuning provides the remaining domain adaptation that zero-shot pre-training alone cannot achieve.

\subsection{Per-Class Topic Analysis}

Table \ref{tab:topic_breakdown} reveals per-class patterns in topic classification across the 7 classes.

\begin{table}[htbp]
\centering
\caption{Per-Class Topic Classification (Multi-Task, 7 Classes: Arts, Business, Fiction, History, Philosophy, Science, Technology)}
\label{tab:topic_breakdown}
\begin{tabular}{lccc}
\toprule
\textbf{Topic} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\
\midrule
Arts & 0.93 & 0.76 & 0.84 \\
Business & 0.97 & 0.97 & 0.97 \\
Fiction & 0.95 & 1.00 & 0.97 \\
History & 0.83 & 0.78 & 0.81 \\
Philosophy & 0.80 & 0.86 & 0.83 \\
Science & 0.58 & 0.73 & 0.65 \\
Technology & 0.86 & 0.89 & 0.87 \\
\midrule
\textit{Macro Avg} & 0.85 & 0.86 & 0.85 \\
\bottomrule
\end{tabular}
\end{table}

Fiction and Business achieve near-perfect classification (F1 $\geq$ 0.97), while Science shows the most confusion (F1 = 0.65). Error analysis reveals Science samples are frequently misclassified as Technology---semantically plausible given that scientific research papers often describe technical methods. The Arts class (which covers visual arts, music, drama, and poetry from Gutenberg subject metadata) shows lower recall (0.76), suggesting some arts-related texts are misclassified into adjacent categories.

\subsection{Analysis: Why Does Emotion Detection Underperform?}
\label{sec:emotion_analysis}

Our emotion sample-averaged F1 (0.20) is substantially lower than reported GoEmotions baselines (0.46 macro F1 with BERT-base \cite{demszky2020goemotions}). We identify four contributing factors, acknowledging that our experimental design does not fully disentangle them:

\begin{enumerate}
    \item \textbf{Domain shift}: GoEmotions labels were annotated on Reddit comments in conversational register. Our encoder is shaped by literary and academic text through the summarization objective, producing representations optimized for formal text. This domain mismatch is likely the largest factor, but we cannot isolate it without a controlled experiment (e.g., fine-tuning BERT on GoEmotions with our frozen encoder vs. BERT's own encoder).
    
    \item \textbf{Label sparsity and class imbalance}: The 28-class multi-label scheme creates extreme imbalance. Rare emotions (grief, remorse, nervousness) appear in $<$2\% of samples. We use a fixed prediction threshold of 0.3 (during training evaluation), without per-class threshold tuning on the validation set---a simplification that the original GoEmotions work \cite{demszky2020goemotions} explicitly optimizes. Per-class threshold tuning could meaningfully improve results.
    
    \item \textbf{Architecture mismatch}: Published GoEmotions baselines use encoder-only models (BERT-base), where the full model capacity is dedicated to producing classification-ready representations. Our encoder-decoder architecture optimizes the encoder primarily for producing representations that the decoder can use for summarization---classification heads receive these representations secondarily. The mean-pooling strategy may also be suboptimal; alternatives such as [CLS] token pooling, attention-weighted pooling, or adapter layers \cite{houlsby2019parameter} could yield better classification features.
    
    \item \textbf{Metric reporting}: We report sample-averaged F1 (per-sample, then averaged), which is not directly comparable to macro F1 (per-class, then averaged) as reported in the original GoEmotions work. Reporting macro F1, micro F1, and per-label performance would provide a more complete picture. We identify this as a gap in our current evaluation.
\end{enumerate}

\textbf{Implication}: Off-the-shelf emotion datasets from social media should not be naively combined with literary/academic tasks in MTL. Domain-specific emotion annotation or domain adaptation techniques are needed for formal text domains.

\subsection{Training Dynamics}

Figure \ref{fig:training_curves} shows training progression over 7 epochs (approximately 6 hours on RTX 4070).

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{figures/training_loss_curve.png}
\caption{Training and validation loss. Best checkpoint at epoch 4; validation loss plateaus from epochs 5--7, triggering early stopping at epoch 7 (patience=3).}
\label{fig:training_curves}
\end{figure}

Key observations:
\begin{itemize}
    \item Topic classification converges by epoch 3 (99\% training accuracy), consistent with the small dataset (3.4K) being memorized quickly. The reduced task weight (0.3) prevents topic gradients from dominating updates.
    \item Summarization loss decreases monotonically through epoch 4, then plateaus (best validation summarization loss: 3.698 at epoch 4).
    \item The train-validation gap widens after epoch 4, primarily driven by topic overfitting on the small dataset. The best checkpoint (epoch 4) balances generalization across all tasks.
\end{itemize}

%=============================================================================
\section{Discussion}
%=============================================================================

\subsection{When Does MTL Help?}

Our results support nuanced, task-dependent guidance:

\textbf{MTL helps when}: A small-dataset task (topic: 3.4K samples) shares domain with a large-dataset task (summarization: 49K literary/academic samples). The topic classifier effectively receives ``free'' pre-training on in-domain text through the shared encoder, benefiting from representations tuned to literary and academic vocabulary and structure.

\textbf{MTL hurts when}: An auxiliary task's domain is misaligned with the primary training signal. Emotion detection, trained on Reddit comments, does not benefit from encoder representations shaped by formal literary/academic summarization. The round-robin scheduling ensures emotion batches receive equal update frequency, but the encoder's representations are skewed toward the summarization domain by gradient magnitude (summarization loss is substantially larger than classification losses).

\textbf{MTL is neutral when}: The primary task (summarization) has sufficient data and a task-specific component (decoder, $\sim$136M parameters) that insulates it from interference. Classification heads are small (single linear layers) and their gradients have limited impact on the shared encoder relative to the decoder's backpropagation signal.

\subsection{Comparison to MTL Literature}

Our findings align qualitatively with several key results in the MTL literature. Standley et al. \cite{standley2020tasks} showed that task groupings critically affect MTL outcomes---we observe this in the contrast between topic (positive transfer) and emotion (negative transfer). Yu et al. \cite{yu2020gradient} demonstrated that gradient conflicts between tasks explain negative transfer; our round-robin scheduling with fixed weights does not address such conflicts, and methods like PCGrad could potentially mitigate the emotion degradation by projecting away conflicting gradient components. Aribandi et al. \cite{aribandi2022ext5} found diminishing or negative returns from adding more tasks in extreme multi-task settings; our small-scale results are consistent with this pattern.

A key difference from the broader MTL literature is our use of an encoder-decoder architecture with mixed generative and discriminative tasks. Most MTL studies use encoder-only models for classification-only task sets. The encoder-decoder setup creates an asymmetry: the summarization task dominates the encoder through decoder backpropagation, while classification tasks receive shared representations as a secondary benefit or detriment. This architectural dynamic deserves further study.

\subsection{Implications for Practitioners}

Based on our findings:

\begin{enumerate}
    \item \textbf{Audit domain alignment} before combining tasks in MTL. If auxiliary tasks draw from different text domains (e.g., social media vs. academic), negative transfer is likely unless mitigated by gradient-conflict methods or per-task adapters.
    
    \item \textbf{Task weighting matters} for preventing small-dataset overfitting. Our reduced weight (0.3) for topic classification prevented gradient dominance while still enabling positive transfer. Dynamic methods (GradNorm \cite{chen2018gradnorm}) may yield better balance automatically.
    
    \item \textbf{Architectural isolation protects high-priority tasks}. Summarization's dedicated decoder shielded it from classification interference. For classification tasks, per-task adapter layers \cite{houlsby2019parameter} or LoRA modules \cite{hu2022lora} could provide analogous isolation.
    
    \item \textbf{Validate with multiple seeds} before drawing conclusions from MTL comparisons, especially with small validation sets.
\end{enumerate}

\subsection{Limitations}
\label{sec:limitations}

We identify several limitations that constrain the generalizability of our findings:

\begin{itemize}
    \item \textbf{Single-seed results}: All experiments are single runs. The +3.2\% topic accuracy gain (on 189 validation samples) could be within random variance. Multi-seed evaluation with confidence intervals is needed to confirm the direction and magnitude of transfer effects.
    
    \item \textbf{No gradient-conflict mitigation}: We use fixed loss weights and do not explore PCGrad \cite{yu2020gradient}, CAGrad \cite{liu2021conflict}, GradNorm \cite{chen2018gradnorm}, or uncertainty weighting \cite{kendall2018multi}. These methods are directly relevant to our observed negative transfer on emotion detection and could potentially convert it to positive or neutral transfer.
    
    \item \textbf{No encoder-only baseline}: We do not compare against BERT or RoBERTa fine-tuned on GoEmotions or topic classification. Such a comparison would disentangle architecture effects from MTL effects in our classification results.
    
    \item \textbf{Emotion evaluation gaps}: We report sample-averaged F1 with a fixed threshold (0.3). Per-class thresholds tuned on validation, per-label metrics, focal loss for class imbalance \cite{lin2017focal}, and calibration analysis would provide more informative evaluation. The conclusion that ``domain mismatch is the primary cause'' of low emotion F1 is plausible but confounded by these design choices.
    
    \item \textbf{No human evaluation}: ROUGE and BERTScore are imperfect proxies for summary quality, especially for creative/literary text where stylistic quality matters beyond semantic accuracy.
    
    \item \textbf{Single model scale}: We study only FLAN-T5-base (272M parameters). Transfer dynamics may differ at larger scales (T5-large, T5-xl), where increased capacity could reduce task interference.
    
    \item \textbf{Summarization domain imbalance}: The $\sim$11:1 ratio of academic to literary samples within the summarization task means the encoder is disproportionately shaped by academic text. This imbalance is not analyzed separately but could affect literary summarization quality.
\end{itemize}

\subsection{Future Work}

\begin{itemize}
    \item \textbf{Gradient-conflict mitigation}: Applying PCGrad or CAGrad to test whether emotion negative transfer can be reduced or eliminated. This is the most directly actionable follow-up given our current findings.
    
    \item \textbf{Parameter-efficient multi-tasking}: Using per-task LoRA adapters \cite{hu2022lora} or adapter layers \cite{houlsby2019parameter} to provide task-specific specialization while maintaining shared encoder representations. This could reduce interference between tasks with misaligned domains.
    
    \item \textbf{Encoder-only comparison}: Fine-tuning BERT/RoBERTa on topic and emotion classification, with and without multi-task training, to disentangle encoder-decoder architecture effects from MTL effects.
    
    \item \textbf{Multi-seed evaluation}: Running at least 3--5 seeds per configuration to establish statistical significance of observed transfer effects.
    
    \item \textbf{Domain-specific emotion annotation}: Collecting emotion annotations on literary and academic text to study whether in-domain emotion data eliminates the negative transfer.
    
    \item \textbf{Improved emotion evaluation}: Per-class threshold tuning, macro/micro F1, class-level analysis, and focal loss to address class imbalance.
\end{itemize}

%=============================================================================
\section{Conclusion}
%=============================================================================

We investigated multi-task learning for literary and academic text understanding, combining abstractive summarization, topic classification, and multi-label emotion detection in an encoder-decoder architecture. Our ablation studies reveal heterogeneous transfer effects: topic classification benefits from shared representations with the larger summarization corpus (+3.2\% accuracy), while emotion detection suffers negative transfer ($-$0.02 F1) due to domain mismatch with Reddit-sourced labels. Summarization quality is robust to multi-task training, insulated by its task-specific decoder.

Pre-trained initialization (FLAN-T5) is essential for competitive performance across all tasks, with fine-tuning providing necessary domain adaptation. These findings are consistent with the broader MTL literature on the importance of task compatibility and domain alignment. However, we emphasize the limitations of our single-seed evaluation design and the absence of gradient-conflict mitigation methods, which could alter the negative transfer findings. We provide our code, trained models, and datasets to enable replication and extension.

Code and models: \url{https://github.com/OliverPerrin/LexiMind}\\
Live demo: \url{https://huggingface.co/spaces/OliverPerrin/LexiMind}

%=============================================================================
% References
%=============================================================================

\begin{thebibliography}{00}

\bibitem{caruana1997multitask}
R. Caruana, ``Multitask learning,'' \textit{Machine Learning}, vol. 28, no. 1, pp. 41--75, 1997.

\bibitem{collobert2011natural}
R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa, ``Natural language processing (almost) from scratch,'' \textit{JMLR}, vol. 12, pp. 2493--2537, 2011.

\bibitem{johnson2017google}
M. Johnson et al., ``Google's multilingual neural machine translation system: Enabling zero-shot translation,'' \textit{TACL}, vol. 5, pp. 339--351, 2017.

\bibitem{mccann2018natural}
B. McCann, N. S. Keskar, C. Xiong, and R. Socher, ``The natural language decathlon: Multitask learning as question answering,'' \textit{arXiv:1806.08730}, 2018.

\bibitem{standley2020tasks}
T. Standley, A. Zamir, D. Chen, L. Guibas, J. Malik, and S. Savarese, ``Which tasks should be learned together in multi-task learning?'' in \textit{ICML}, 2020.

\bibitem{yu2020gradient}
T. Yu, S. Kumar, A. Gupta, S. Levine, K. Hausman, and C. Finn, ``Gradient surgery for multi-task learning,'' in \textit{NeurIPS}, 2020.

\bibitem{liu2021conflict}
B. Liu, X. Liu, X. Jin, P. Stone, and Q. Liu, ``Conflict-averse gradient descent for multi-task learning,'' in \textit{NeurIPS}, 2021.

\bibitem{chen2018gradnorm}
Z. Chen, V. Badrinarayanan, C.-Y. Lee, and A. Rabinovich, ``GradNorm: Gradient normalization for adaptive loss balancing in deep multitask networks,'' in \textit{ICML}, 2018.

\bibitem{kendall2018multi}
A. Kendall, Y. Gal, and R. Cipolla, ``Multi-task learning using uncertainty to weigh losses for scene geometry and semantics,'' in \textit{CVPR}, 2018.

\bibitem{aghajanyan2021muppet}
A. Aghajanyan, A. Gupta, A. Shrivastava, X. Chen, L. Zettlemoyer, and S. Gupta, ``Muppet: Massive multi-task representations with pre-finetuning,'' in \textit{EMNLP}, 2021.

\bibitem{aribandi2022ext5}
V. Aribandi et al., ``ExT5: Towards extreme multi-task scaling for transfer learning,'' in \textit{ICLR}, 2022.

\bibitem{raffel2020exploring}
C. Raffel et al., ``Exploring the limits of transfer learning with a unified text-to-text transformer,'' \textit{JMLR}, vol. 21, no. 140, pp. 1--67, 2020.

\bibitem{chung2022scaling}
H. W. Chung et al., ``Scaling instruction-finetuned language models,'' \textit{arXiv:2210.11416}, 2022.

\bibitem{nallapati2016abstractive}
R. Nallapati, B. Zhou, C. dos Santos, C. Gulcehre, and B. Xiang, ``Abstractive text summarization using sequence-to-sequence RNNs and beyond,'' in \textit{CoNLL}, 2016.

\bibitem{narayan2018don}
S. Narayan, S. B. Cohen, and M. Lapata, ``Don't give me the details, just the summary! Topic-aware convolutional neural networks for extreme summarization,'' in \textit{EMNLP}, 2018.

\bibitem{kryscinski2021booksum}
W. Kryscinski, N. Rajani, D. Aber, and C. Xiong, ``BookSum: A collection of datasets for long-form narrative summarization,'' in \textit{Findings of EMNLP}, 2021.

\bibitem{cohan2018discourse}
A. Cohan et al., ``A discourse-aware attention model for abstractive summarization of long documents,'' in \textit{NAACL-HLT}, 2018.

\bibitem{mao2022citesum}
Y. Mao, M. Zhong, and J. Han, ``CiteSum: Citation text-guided scientific extreme summarization and domain adaptation with limited supervision,'' in \textit{EMNLP}, 2022.

\bibitem{demszky2020goemotions}
D. Demszky et al., ``GoEmotions: A dataset of fine-grained emotions,'' in \textit{ACL}, 2020.

\bibitem{zhang2019root}
B. Zhang and R. Sennrich, ``Root mean square layer normalization,'' in \textit{NeurIPS}, 2019.

\bibitem{lin2004rouge}
C.-Y. Lin, ``ROUGE: A package for automatic evaluation of summaries,'' in \textit{Text Summarization Branches Out}, 2004.

\bibitem{zhang2019bertscore}
T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi, ``BERTScore: Evaluating text generation with BERT,'' in \textit{ICLR}, 2020.

\bibitem{hu2022lora}
E. J. Hu et al., ``LoRA: Low-rank adaptation of large language models,'' in \textit{ICLR}, 2022.

\bibitem{houlsby2019parameter}
N. Houlsby et al., ``Parameter-efficient transfer learning for NLP,'' in \textit{ICML}, 2019.

\bibitem{lin2017focal}
T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll\'{a}r, ``Focal loss for dense object detection,'' in \textit{ICCV}, 2017.

\end{thebibliography}

\end{document}
